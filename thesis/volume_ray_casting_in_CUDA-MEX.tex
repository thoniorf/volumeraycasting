\documentclass[12pt,a4paper]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\newcommand{\row}[1]{% inline row vector
  \begin{matrix}(#1)\end{matrix}%
}
\author{Antonio Fortino}
\title{Volume ray casting accellerated with CUDA in Matlab MEX context}
\begin{document}
\maketitle
\begin{center}
Academic Year 2018/2019\\
University of Applied Science Upper Austria Hagenberg Campus\\
\&\\
University of Calabria
\end{center}
\pagebreak
\tableofcontents
\pagebreak
\section{Introduction} %TODO 
\pagebreak
\section{Basics of computer graphics and \\visualization}
\subsection{Computer graphics concepts} %TODO 31
\subsubsection{Rendering pipeline} %TODO 30-31
\subsubsection{Affine transformations}
One fundamental concept in computer graphics and more specifically in 3-Dimensional computer graphics applications, is transformation matrix. However, the right of definition of a transformation matrix is a affine transformation.
In geometry, an affine transformation is a function between two affine space, such as two Euclidean space, that transform points and lines and other geometrical objects from one space to the other one, preserving them. For example, two parallel lines in an Euclidean space will remain parallel even if an affine transformation in applied and the Euclidean space is changed.

Indeed, an affine transformation is defined as such if and only if, it respect a set of properties, listed below:
\begin{itemize}
\item Collinearity between points: three or more points must continue to be collinear after the transformation. %TODO ref collinear
\item Parallelism: tow or more lines which are parallel, continue to be parallel after the transformation.
\item Convexity of sets: A convex set continues to be convex and the extreme points of the original set are mapped to the extreme points of the transformed one.
\item Ratios of lengths of parallel lines segment: distinct parallel segments which ratio of length of the original point is the same as the ratio of the length of the point applying the transformation individually.
\item Barycenters: the barycentre of a weighted collections of points remains the same after the transformation.
\end{itemize}

In the context of computer graphics, such affine transformation matrices are defined a $4x4$ matrix. The reason of this choice is that in order to encode all different type of transformation that can applied to a point into one matrix in a 3-Dimensional space, an additional row and as in computer graphics perspective is taken also into account, an additional column must be added.

There exists different affine transformation which are very common and useful in computer graphics application:
\paragraph{Identity transformation} The identity transformation is usually used as a starting point as it leaves the point completely unmodified. The matrix of the identity transformation is stated below:
\[
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}
\begin{pmatrix}
x\\
y\\
z\\
1\\
\end{pmatrix}
=
\begin{pmatrix}
x\\
y\\
z\\
1\\
\end{pmatrix}
\]

As can be seen, the vector encoding the position in the 3-Dimensional space in left untouched. However, such vector is composed of four components and not three as it should be. As previously mentioned, affine transformations are encoded in $4x4$ matrices.

 Therefore, in order to be able to perform multiplication between a position vector and a transformation matrix an additional dimension must be added. Such new component is known as \textbf{homogeneous point} (or a point with \textbf{homogeneous coordinates}, that usually assume value 1, besides when not very common transformation are applied like the shear transformation or when dealing with perspective projection.

\paragraph{Scale transformation} The scale transformation is used unsurprisingly to scale-up and scale-down the magnitude of the vector but preserving its direction. There exists two type of scaling operation: \textbf{non-uniform scale} and \textbf{uniform scale}. It depends on the scaling factor applied to the different dimension, if the scalar would be equal on all axes(X, Y and Z as it is a 3-Dimensional space) it would be an uniform scale, otherwise it would be a non-uniform scale operation. 

An example of the matrix that encodes this transformation can be seen below, using $S_{1}$,$S_{2}$ and $S_{3}$ to represent the scaling variables:
\[
\begin{pmatrix}
S_{1} & 0 & 0 & 0\\
0 & S_{2} & 0 & 0\\
0 & 0 & S_{3} & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}
\begin{pmatrix}
x\\
y\\
z\\
1
\end{pmatrix}
=
\begin{pmatrix}
S_{1} \cdot x\\
S_{2} \cdot y\\
S_{3} \cdot z \\
1
\end{pmatrix}
\]
\paragraph{Translation} The translation transformation will move an point in any of the three direction by means of the translation vector. In previous examples the column-major has been used, but for the sake of simplicity vectors will be expressed as row-major order.  %TODO ref row and colum order. 
Such vector is it encoded into the transformation matrix as follows:
\[
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 1\\
0 & 0 & 1 & 0\\
T_{x} & T_{y} & T_{z} & 1\\
\end{pmatrix}
\row{x&y&z&1}
 = 
 \row{x+T_{x}&y+T_{y}&z+T_{z}&1}
\]
\paragraph{Rotate transformation} There exists different rotate transformation matrices, each of which works with respect to one axis because space rotations, in a 3-Dimensional, are specified with a rotation angle and a rotation axis.

Therefore there is the rotation matrix around the X-axis, the rotation matrix around the Y-axis and the rotation matrix around the Z-axis. Such transformations are done by means of using trigonometry functions \textit{sine} and \textit{cosine}.

Rotation around X-Axis:
\[
\begin{pmatrix}
1&0&0&0&\\
0&\cos(\theta)& -\sin(\theta) & 0\\
0&\sin(\theta)&\cos(\theta)&0\\
0&0&0&1\\
\end{pmatrix}
\]
Rotation around Y-Axis:
\[
\begin{pmatrix}
\cos(\theta) & 0 &-\sin(\theta)& 0\\
0&1&0&0\\
\sin(\theta)&0&\cos(\theta)&0\\
0&0&0&1\\
\end{pmatrix}
\]
Rotation around Z-Axis:
\[
\begin{pmatrix}
\cos(\theta)& -\sin(\theta) & 0 & 0\\
\sin(\theta)&\cos(\theta)&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix}
\]
Using the rotation matrices we can then transform a position vector around one of the three axes. However, in order to combine and perform more complex transformation they must be applied following a certain order: rotating first around the Y-Axis, the around the X-Axis and finally around the Z-Axis. Otherwise, the resulting combined matrix will returns an unwanted and incorrect rotation of the a given point.
\vspace{8pt}

The last concept that goes along with affines transformations and that is present in every computer graphics application, in some form or another is \textbf{the camera}. Indeed, it is also included in the implementation part of this work as usually, cameras are the first means of interaction of users with a 3-Dimensional scene %TODO ref scene
in computer graphics, using mouse, keyboard or other graphical widgets available.

The reason why it is explained along with affine transformations is because the two concepts are strictly related with each other. A camera is none other than an object that moves around the world and project the objects that falls under its visibility box, called frustum. %TODO ref frustum and add photo

In order to define a camera few components must be defined first, because cameras are defined by a front vector, an up vector and a right vector, beside the usual position vector that is part of other \textit{'non special'} objects. These vectors all together defines the direction, orientation, rotation and of course the position of a camera.
There exist one easy and immediate way of encoding all these vector together, by means of a matrix known as \textbf{lookup matrix}.

The technique to build such a lookup matrix is known as Look-At. The idea is that in order to set the camera position and the orientation,two point in the space are needed: one to actually position the camera, called \textit{'from'} and a point that defines what the camera is looking at, called \textit{'to'}.

The lookup matrix that is used to control a camera, is defined as follows:
\[
\begin{pmatrix}
Right_{x} & Right_{y} & right_{z} & 0\\
Up_{x} & Up_{y} & Up_{z} & 0\\
Forward_{x} & Forward_{y} & Forward_{z} & 0\\
T_{x}& T_{y} & T_{z} & 0\\
\end{pmatrix}
\]

The method to build this lookup matrix from the from-to pair of points can be broken down in four steps:
\begin{itemize}
\item \textbf{compute the forward axis}: The forward axis is aligned with the segment defined by the points from and to. Therefore, the forward vector is computer as: 
\[
Forward = \frac{To - From}{|To-From|}
\] 
The normalized vector of the difference between the vector \textbf{to} and the vector \textbf{from}.
\item \textbf{compute the right vector}: The right vector is perpendicular to the forward vector. Therefore, it can be computed as the result of a cross product between the forward vector and another vector such as the Up vector. 

However, as the Up vector is still unknown, a new temporary vector must be introduced. 

Usually, such temporary vector is either one the unit vectors that represent on of the axis, such as the Y-Axis unit vector $\begin{pmatrix}
0 & 1 &0
\end{pmatrix}$. Therefore, the right vector can be calculate as follows:
\[
Right = \begin{pmatrix}
0 & 1 & 0
\end{pmatrix}
\times Forward
\]
\item \textbf{compute the up vector} As the Right vector, computing the Up vector is easy as the cross product between the Forward vector and the Right vector, because they are orthogonal so computing the cross product will give the missing third vector.

Therefore, the Up vector is equal to:
\[
Up = Forward \times Right
\]
\item \textbf{Build the lookup matrix} : the lookup matrix can be built by just replacing the computed vectors like the matrix above:
\begin{itemize}
\item First row: Right vector
\item Second row: Up vector
\item Third row: Forward vector
\item Fourth row: From point vector
\end{itemize}
\end{itemize}

It is important to point out that this method as some limitations that must be taken into account. The weakness of the LookAt method is that when the camera is vertical looking straight down or straight up, the forward axis gets very closes to the arbitrary axis used to compute the right axis. 

Unfortunately, in this particular case, the cross product fails producing a result for the Right vector, so it also fails producing a result for the Up vector. There exists an elegant solution such as using quaternion interpolation. 

However, for the sake of simplicity, in this work this edge case has been handle with ad hoc solution by changing the arbitrary axis, as it can be seen in the implementation.
%lookup matrix
\subsubsection{Coordinates systems}
In computer graphics every object is positioned in the world and therefore it has some coordinates that let its position defined and retrieved. Such coordinates can resides both in 2-Dimensional or 3-Dimensional space. Mathematically speaking, as coordinate system uses ordered coordinates to uniquely determine the position of the points or other geometric elements on a manifold such as Euclidean space. %TODO cite wikipedia coordinate system refs and ref euclidean space.

%TODO right hand - left hand
Usually with computer graphics the first kind of coordinates that comes into people mind are 3-Dimensional. Indeed, usually the world coordinates system is the first one that is recollected as a direct consequence of 3-Dimensional computer graphics software in the people's imaginary collective.


However, there exists different type of coordinate system inside a single computer graphics application, besides World space system, such as Local space system, View space and Screen space, but not limited to as it also depends of the specific implementation,; for example application made with OpenGL framework also deals with clip space.
\begin{itemize}
\item World space: Coordinates in this system are exactly what they sounds like, coordinates that are relatives to a global origin from which every object refer to, usually $(0,0,0)$.
\item Local space: Coordinates of this system are local to the object, this means that they refers to the origin of your object.
\item View space: The view space is what people usually refer to as the camera. Coordinates in this system are the result of the transformation from world space that are in front of the user's view (or eye).
\item Screen space: Defines the coordinates inside the range of the screen size with respect to its width and height in terms of pixels (eg. $1920$ x $1080$ ).
\end{itemize}

In order to properly render an image on the screen, independently of the actual implementation, several transformation must be performed. The reason is that, objects are first defined inside the local space, then such coordinates must be transformed to world space coordinates to be able to build a space relation between other objects. Then View and Screen space coordinates are need to correctly place the object into our 2-Dimensional space in which every image on the screen resides.


To transform the coordinates in one space to the next coordinate space we'll use several transformation matrices, also known as affine transformation, as already explained in the previous section.
Here it is possible to have a global view of the proceed and the different transformation matrices that are applied to an object to be rendered on the screen. %TODO put image learn opengl

%TODO transfomation matrices
\pagebreak
\subsection{The illumination model} %TODO 30
Illumination models are the very important in computer graphics as they let rendering and visualization feels seamless and convincing. %TODO nel contesto del medical imaging
There differente illumination model, such as Phong, Blinn-Phong, Ray tracing, another one. However in this section the focus will be concentrated on the Phong and Blinn-Phong models; the latter is an improved version of the first one that solves some light-related artefacts.
%TODO phong formula
%CONS
%TODO blinn-phong formula
%PROS
\subsubsection{Color spaces} %TODO 29
Color spaces are one fundamental aspect of computer graphics and visualization in general.
A colour space defines a model in which Color are organized, usually in a 3-Dimensional space.
A colour model is an abstract mathematical model that describe the way Color can be represented as tuples of numbers; typically, such tuples are made of three components, also known as color component within the context of Color space.
A 2-Dimensional representation of the range of possible colors that can be represented by a particular system is called the \textbf{garmut}.


There exists different color spaces, each of which is well defined by some standard and its primary use for which it has been invented in the first place, as usually colors spaces were built with some technology standard and devices used in different part of the world, such as NTSC or PAL color system standard for televisions and screens.


In the context of computer graphics and visualization of this work, there are few notable color spaces worth mentioning that can be used along the different techniques that will be described in the next sections, and these are:
\begin{itemize}
\item \textbf{RGB} : it stands for Red, Green and Blue, that are the main colors used in combination to generate all other possible available colors in the garmut of the space.
\item \textbf{YUV} and \textbf{YCbCr} : These are two standard used with old screens and with HDTV respectively. They belong to a broader category of color spaces known as Luminance plus chroma or chrominance color space. The main advantage is that the real color is encoded in the two chrominance values and the luminance override the color brightness.
\item \textbf{HSV} and \textbf{HSL} : These two standards can be seen as a direct transformation from the RGB color space. HSV stands for Hue, Saturation and Value, while HSL stands for Hue, Saturation and Luminance; they are used often because it a more natural way of dealing with with colors in terms of hue and saturation rather then in terms of adding or subtracting color component as in RGB space.
\end{itemize}
In the next paragraph will be describe how to go from one color space to another one. Specifically, as each color space has its own characteristic and mathematical foundations, focus will be narrowed on the color space that is also used in this work, beside RGB, that is YUV color space describing how it is defined and which is the proceed to convert a color defined in the RGB color space to a color defined in the specific color space.


In particular the technique to go from and back between the RGB color space and the YUV color space has been used and implemented in this work during the shading phase of the proposed algorithm, which is described in Section 3. %TODO section 3 ref 
\paragraph{From RGB to YUV and back} First of all, as already mentioned, the YUV color space defines a color is terms of Luminance(Y) and chrominance(UV). YUV was invented during the transition between only black-and-white television and color television. Indeed, at the beginning only the Luminance component was transmitted and the colors were sent by means of separate signals in order to be able to visualize colors in a black-and-white architecture.


There exists different defined standards to convert a color from RGB to YUV and vice versa: \textbf{BT.601}, \textbf{BT.709} and \textbf{BT.2020}, developed for SDTV, HDTV and  UHDTV respectively.The main difference between these standards resides in the range breadth of the relative garmut, as UHDTV has a bigger and wider garmut then it comes HDTV and in the end SDTV.They all share the common concept that the Y component of YUV represent a measure of overall luminance or brightness and therefore it is computed by applying different weights to the values of Red, Green and Blue of the RGB color space.

Then, U and V chrominance values are compued as scaled differences between the luminance Y of YUV and the Blue and Red values of RGB.
Each standards defines its own constants and weight to perform the transformation from RGB.
Therefore, in order to convert from one color space to the other the weighting values must be defined, and these values are :
\begin{itemize}
\item $W_{R}$ : the weight for the red value of RGB and it is defined a constant value.
\item $W_{B}$ : the weight for the blue value of RGB, defined also as a constant value.
\item $W_{G}$ : the weight for the green value, defines by the following formula
\begin{center}
$1-W_{R}-W_{B}$.
\end{center}
\item $U_{max}$ : it represent the maximum value of the U chroma component.
\item $V_{max}$ : the maximum value of the V chroma component.
\end{itemize}
Then, a YUV color is computed from linear RGB %TODOref to gamma corrected and sRGB
applying these weight to the RGB components.


The luminance component Y is computed by means of the following formula:
\[Y = W_{R}R + W_{G}G + W_{B}B\]
U and V chromatic components can be computed as follows:
\[U = \frac{B-Y}{1-W_{B}} U_{max}\]
\[V =\frac{R-Y}{1-W_{R}} V_{max}\]
Equivalently, it is possible to built a matrix that represent the transformation by substituting values for constants and expressing the color component as a Vector, obtaining a matrix vector multiplication like the one below, that refers to the transformation matrix for BT.601 standard:
\[
\begin{pmatrix}
Y\\
U\\
V\\
\end{pmatrix}
=
\begin{pmatrix}
W_{R} & W_{G} & W_{B}\\
-0.14713 & -0.28886 & U_{max}\\
V_{max} & -0.51499 & -0.10001\\
\end{pmatrix}
\begin{pmatrix}
R\\
G\\
B\\
\end{pmatrix}
\]
The resulting values of Y, U and V will lie inside the respectively range of:
\begin{center}
$[0,1]$, $[-U_{max},U_{max}]$, $[-V_{max},V_{max}]$
\end{center}


The advantage of representing the transformation as a matrix and matrices operations is that it becomes easier to perform the proceed backward, from YUV to RGB color space.
Indeed, in order to convert a color specified in the YUV color space to a color in the RGB color space the inverse matrix of one above can be used, like the following:
\[
toRGB^{-1} = \frac{1}{|toYUV|}
\begin{pmatrix}
W_{R} & W_{G} & W_{B}\\
-0.14713 & -0.28886 & U_{max}\\
V_{max} & -0.51499& -0.10001\\
\end{pmatrix}
\]
Then, as it has been done before from RGB to YUV, the inverse matrix in multiplied with the vector that encode the color components in YUV color space, like the following:
\[
\begin{pmatrix}
 R\\
 G\\
 B\\
\end{pmatrix}
=
toRGB^{-1}
\begin{pmatrix}
Y\\
U\\
V\\
\end{pmatrix}
\]
In the end, as it will be shown later on, in the Implementation section of this work, such YUV color components that are converted to RGB color space may end up resulting invalid, as outside the defined range $[0,1]$ of RGB color. Therefore they must be clamped between $0$ and $1$, in order to have correct and valid RGB color.
%\paragraph{From RGB to HSL and back}

\pagebreak
\subsection{Volume rendering} 
This chapter makes a short introduction to different type of techniques and approaches to render a a 3-dimensional data, also known as volume and some fundamental concepts. Then, few sections will be reserved to a more in-depth explanation of some of the most common techniques along with advantages and disadvantages, in order to understand the reasons behind some decision that has been taken for this work. In the last section of this chapter, there will be a description of the current state-of-the-art volume rendering techniques used nowadays.

Volume rendering is a subject of computer graphics that involves volumetric datasets as source. Fundamental concept in volume rendering is the voxel. 
A voxel act as the basic unit for volumes, as pixels for 2-dimensional images.
It represent a single value on a regular grid in 3-dimensional space (volume), and they can represent different values such as MRI and ultrasound samples or values stored in CT scans. Voxels position in space is not explicitly encoded in the voxel itself, instead it is inferred with respect to the other voxels in the grid forming the whole volume which position is define in space. In contrast with points and polygons that also encode their positions. %TODO reference wiki or somenthing else 

There exists different ways to visualize and render a volumetric dataset such as x-Rays, tomography and such; they differ from each other in the way the volume is approached: directly or indirectly.
Indirect Volume rendering techniques, as the name implies, before the actual render of the volumetric dataset, they introduces an intermediate stage in which the source data in transformed, and then it is rendered by means of common techniques, such as rasterization et al; for this reason, such techniques are also known as surface rendering techniques.
The main idea of those technique is to go through the volumes' voxels and determines if they belongs to a certain isosurface with a specified values. Therefore, they rely on differentiable functions.\\
Common techniques that belongs to the indirect volume rendering category whom reconstruct surfaces from volumetric data are:
\begin{itemize}
\item Contour tracing
\item Marching cubes
\item Marching tetrahedra
\end{itemize}

Nowadays, the last twos are known to be the most used indirect volume rendering techniques.
They are often chosen due to their computational speed %TODO cita VIS-MODULE-05-Volume_visualization
and less storage space requirements, over plain implementations of the other techniques. However, they suffer of a main disadvantage that can't be completely ignored in the context of medical imaging as aid in diseases diagnosis or even in surgery rehearsal, whom are the main context of this work, that is the lack of and high level accuracy and precision; it doesn't means they are not valid at all, but by means of other approaches initially slower better results can be archived with respect to the patient condition stored in the source dataset.  

On the other side, we do have Direct Volume rendering technique which visualize the volumetric data, directly reading the sampled values stored into voxels and display them, without producing any intermediate surface, unlike indirect approaches do. These techniques can be furthermore distinguished in image-based and object-based. Image-based techniques starts from the 2-dimensional view space and their computation is performed pixel by pixel.In contrast, object-based techniques' approach start from the objects stored into the volume. There exists different, and very common, direct volume rendering techniques :
\begin{itemize}
\item Image-based approach:
\begin{itemize}
\item Ray casting
\end{itemize}
\item Object-based approach:
\begin{itemize}
\item Shear-Warp
\item Splatting
\item Texture-based
\end{itemize}
\end{itemize}

One of the main disadvantage of these techniques is that, their computational speed is pretty low compared to the indirect volume rendering competitors. However, Ray casting especially, is able to often output better results in terms of source fidelity, and also because ray casting and the others direct volume rendering technique allows to visualize both the interiors and the exterior of the volume unlike the others approaches.

\subsubsection{Surface rendering} %TODO 24
This section will focus on the fundamental concepts of the common techniques that belongs to indirect volume rendering category. First a brief explanation of the Contour tracing algorithm, mentioned in the previous section, used during the early days of volume rendering in medical contexts. Then the core idea of Marching cube technique will be described, as it is one of the main competitor in his category used in state-of-the-art indirect volume rendering techniques, nowadays.
\paragraph{Contour tracing}
Contour tracing was one of the first approach studied in early days of medical imaging in the digital era and different techniques has been developed and proposed during, years each of which tried to solve some others lacks, such as :
\begin{itemize}
\item Square tracing
\item Moore-Neighbor Tracing
\item Radial Sweep
\item Theo Pavlidis' Algorithm 
\end{itemize}
The differences between them lies in how the concept how adjacency is defined.
%TODO cita http://www.imageprocessingplace.com/downloads_V3/root_downloads/tutorials/contour_tracing_Abeer_George_Ghuneim/alg.html
Nevertheless, the underlying concept is shared among those algorithms.

The core idea behind contour tracing techniques is to identify and extract the boundaries of a given digital image; within the context of medical imaging and volume rendering means: identify different tissue/region of a given volumetric data, applying the algorithm for each slice that forms the volume. 

The concept can be described, in a very simplified and generic way, as a local operator %definire local operator
applied to a single slice that build a polyline made of vertex by traversing the adjacent pixels in the 2-Dimensional space of the slice, which should belong to the contour that defines a region of that slice, following a specific traversing order clockwise or counter-clockwise. As most of the techniques in this field of studying, a threshold value guide defines the which pixels values must be considered and which one must be discarded.

Something that is worth to mention is that, even if the generality of contour tracing techniques approach proceed in clockwise direction, Theo Pavlidis in his proposed version decided to approach in a counter-clockwise manner and he also defined few initial restriction conditions with respect to how the starting point is chosen, as he imposed that a starting pixel can be choose if and only if its left adjacent neighbor doesn't belong to the same pixel pattern.

In the end, those algorithms must undertake further steps to build the isosurface needed from a 2-Dimensional polyline contour; these steps can be summarized as follow:
\begin{itemize}
\item Labeling by identification of different structures within the same slice, defined by the isovalue and sort them from the highest to the lowest order of characteristics.
\item Tracing the contour from adjacent slices that represent the same object and connect them forming triangles.
\item Rendering the triangles
\end{itemize}


Contour tracing suffer of different problems that are related to the intrinsic  proceeding way of the algorithm.

The first, non-negligible, problem is related to complex pixel patterns, such as the one that presents holes, as this techniques is not able to recognize holes at all, ending up ignoring them completely.

The second problem is related to the contour density in each slice and the tracing step, previously mentioned; Sometimes there are a lot of different contour and therefore different object inside a single slice or there is an high variation of them between slices, therefore it is very difficult to trace contours from one slice to the other, so most of the computation time is spent connecting the vertices of triangles from different heights.

In order to overcome these important failing at some point in time in 1987, a new technique has been proposed that still provides good results, as it has been describe in the next paragraph.
  

\paragraph{Marching cubes}
Marching cubes algorithm render the data set, sending through the volume cubes with different configuration whom while traversing it, they depict the actual volume shape. Indeed, the output served by Marching cubes algorithm is a polygonal mesh of an isosurface computed from a 3-Dimensional discrete scalar dataset. It was first proposed and developed by E.Lorensen and H. E. Cline. The input volume is subdivided in a grid of cubes, then values that falls beyond the isosurface threshold are interpolated with respect to vertices of the grid of cubes and a pre-computed array of vertices configurations.

Those configurations are $256$ as $2^8 = 256$ and their purpose is to check for every cube of the grid, the correct configuration interpolating the grid vertex that falls into the isosurface. In this way, each time a configuration matches the corresponding interpolated vertex defined by the configuration are used to represent the surface.\\
The marching cubes algorithm proceed can be summarize as follows:
%TODO cite visualization handbook page 8?
\begin{enumerate}
\item Select a cell of the grid
\item Compute the inside/outside state of each vertex of the cell.
\item Create an index storing the state of each vertex in a separate bit
\item Use the index to retrieve the corresponding edges from the lookup table of configurations.
\item Linearly interpolate the founded edges to calculate the contour
\item Compute the gradient
\item Move to the next cell
\end{enumerate}

The proposed algorithm from Cline and Lorensen exploiting rotational and reflective symmetry along with changes in sign of the cubical configurations, the number of total configuration is narrowed down to 16 only. However, one of the main disadvantages is that it present some discontinuities and topological issues, due to some ambiguities in the trilinear interpolation of the cubes that occurs in scenarios where there aren't sufficient vertex to determine the correct surface triangulation. %TODO cite original and wikipedia

Initially, marching cubes algorithm was protected by software patent, therefore an alternative was developed that didn't required any license and that solves the aforementioned ambiguity for some cubes configuration.

\paragraph{Marching tetrahedra} works the same as marching cubes. However it solves the topological issues and discontinuities discussed before, giving in output almost always topological correct and watertight results.
The basic idea, that in the end enchanted the results, is to improve the cubic configurations by making addition cuts breaking the cube into several tetrahedra.

The advantage of using tetrahedra is that the are no more ambiguity with respect to the configurations. As only four vertices rather than eight are involved, the number of configuration determining the size of the lookup tables is much smaller. However, there two downsides of this approach:

The first  is that Marching tetrahedra produces more triangles that aren't always needed for correctly rendering a volume; 

The second is that an orientation of the tetrahedra must be defined and can produce bumps in the final produced surface,because of interpolation along the face diagonals.

\subsubsection{Direct volume rendering} %TODO 25-26
This section will describe, as it has been done in the previous section for indirect volume rendering techniques, direct volume rendering techniques. Briefly going from object-based direct techniques previously mentioned, such as Shear-Warp, Splatting and Texture-based, to image-based approach volume ray casting technique, with a deep explanation of the main concepts, its first proposed version and today still valid.

As already said there exists different type of algorithms to directly render a volumetric dataset, without going through an intermediate layer like a mesh.
Furthermore, they can be divided int two categories: object-based or image-based, depending on the starting space, object space or view space.
%TODO add volume integral
%TODO add common steps definitions

\paragraph{Shear-Warp} Shear-Warp was introduced by Lacroute, %TODO cite lacroute 
this technique combines some of the advantages of both object-based and image-based algorithms. IT is very efficient because it combines the object-order traversal and the scan-line order sampling.
It basically, project the volume from the object space to the image space in a way that the voxel grid is aligned with the viewing plane.
In order to archive this results, it applies different transformations in a specific order, also known as affine transformations.
Therefore,the algorithm proceed can be summarize as follows:
\begin{enumerate}
\item \textbf{Shear:} Transform the volumetric data into sheared object space, by translating and re-sampling the voxels, according to the Shearing ($S$) transformation matrix. In case of perspective view, scale each slice.
\item \textbf{Project (3D $\rightarrow$ 2D):} Composite the resampled slices following front-to-back order, applying the over operator (see Composition schemas section).
\item \textbf{Warp:} Warp the intermediate result image to the view space, to produce the correct finale output image.
\end{enumerate} %TODO insert image of transformation matrix

The reason why Shear-warp is a very fast techniques and is also used in state-of-the-art direct volume rendering algorithm, is due to some characteristics of sheared object space in which the slices of the volume data are transformed to.
These properties are the following:
\begin{itemize}
\item The scan lines %TODO ref scan line
of pixels in the intermediate image are parallel to scan lines of voxels in the volume.
\item All voxels in a given slice are scaled by the same factor. Furthermore, if no perspective transformation is involved, all voxels have the scale factor.
\end{itemize}
Therefore, the composition is simplified a lot, as there exists a one-to-one mapping between voxels and intermediates pixels, this means that both voxels scan lines and pixel scan lines of the intermediate image can be traverse at the same time, following scan line order.

\paragraph{Splatting} Splatting is another well known direct volume rendering technique that belongs that belongs to the object-base category. It can be summarize as an algorithm that distribute values of the volume data onto a view plane with different shape and following a distribution function.\\ The basic idea is to traverse the slice in a front-to-back manner and for each voxel compute which pixel of the image plane is covered by the voxel \textit{"footprint"} resulting from \textit{"throwing"} voxels toward the image plane; such \textit{"footprint"}, are 3D interpolation kernels %TODO 3d interpolation kernel
and can have different shapes and opacity, usually they are similar to the Gaussian function; there exists different possible choices, but they don't give satisfying result as Gaussian.

The algorithm can be summarize as follow:
\begin{enumerate}
\item For each voxel in each slice add a corresponding 3D interpolation kernel.
\item For each voxel add the corresponding kernel to a 2-Dimensional bugger plane, known as buffer-sheet.
\item Accumulate the buffer-sheet values into the compositing buffer, the final output image. 
\end{enumerate}

These techniques present some advantages: It is quite fast because interpolation is operated in 2-dimensional space; Can be easily parallelized;
However, there are also different disadvantages: When to kernels overlaps, aliasing occurs leaving gaps on the output image; When zoomed in, the final image appears to be blurred, as the Gaussian function cuts away higher frequencies.

\paragraph{Texture-based} Texture-based techniques are quite famous because they allows to exploit existing hardware capabilities and rendering techniques. There exists to type of texture-based algorithm: 2-D texture slicing and 3-D texture slicing. However, 2-D texture still outperform the latter for large volume and that's the reason why 3-D texture slicing will not be inside the focus of this explanation, as the main objective of this works is to handle large data set efficiently.
The core concept of 2-D texture is to create slices of the volume, create polygons out of them and take advantage of existing implemented funcionality on graphics hardware for interpolation and compositing, such as: Rasterization, Texturing and Blending, available in almost all modern GPUs.

The simplest implementation of 2-D texture slicing involves storing three different set of slices, one per each 2-Dimensional plane of the coordinate system (XY,YZ,XZ) and it requires interpolating the volume to get these sets.
The second step is to load each 2D slice of a set into the texture memory buffer;  Simple squared (Four vertices) polygons are created for each slice and the corresponding slice texture stored in the buffer is applied by means of mapping
%TODO ref texel
texel coordinates to vertex coordinates.

All the polygons are then , rendered and blended using common techniques, as already mentioned, such as rasterization and blending provided for example by OpenGL or other computer graphics libraries

%TODO show image from texure slicing
The choice of which slice set has to be selected and used during the proceed is determined by the angle and position of the view vector with respect to the volume center origin.

In the end, the 2-D texture slicing technique can be summarized as follows:
\begin{enumerate}
\item Create slices sets one for each 2-Dimensional plane : XY, YZ, XZ.
\item Depending of the viewing angle and position, choose the closest sets of slices (the ones that require less affine transformations to be perpendicular to the view)
\item For each slice, store the values as separate textures in the texture buffer of the graphics device.
\item Create one polygon for each slice in the set.
\item Map texture coordinates to four polygon vertices
\item Render and blend the polygons
\end{enumerate}

The main advantage of this approach is that it exploit graphics hardware broad rendering capacity and already available algorithms. However, there are few major disadvantages: The first one is the texture buffer memory, that can become a bottleneck of the algorithm as it can be saturated by very large volume data set; The other problem becomes evident when the viewing angle is moved to 45 degree with respect the current selected set of slices, because it is required to switch to another set and apply new transformations to made it perpendicular again.
Last but not least disadvantage is the needs of three different sets of slices that requires additional storing memory and a preliminary step of interpolation of the volume.

\paragraph{Volume Ray casting}
The most known direct volume rendering algorithm is Volume Ray casting,a first version along with an implementation has been proposed by %TODO add proposed%.
Levoy introduced Volume ray casting as a ray tracing technique involving volumetric data set and it is not in the surface-based rendering. However, the naming convention is evolved as the time comes, separating the two techniques.

Indeed, the core concept is very similar to the one used in the ray tracing technique beside that in ray tracing multiple types of ray are sent, known as primary rays, secondary rays and so on, based on the source from which they are generated. Another, very clear difference is that ray tracing deals with triangles, quads and polygon in general, as already said, Volume ray casting deals with volumetric dataset composed by voxels, instead.

The objective of the volume ray casting technique is to transform a 3-Dimensional volumetric dataset in a 2-Dimensional projection of the latter in the form of digital image.

Volume ray casting technique can be decomposed in four different steps %TODO state wikipedia
: cast and traverse, sample color and opacity, compute shadow and store the final result in a 2D view plane, that represent the final output image.
The starting point of the algorithm is the observer view point, or the camera center point as in development environment such as OpenGL or similar, for this reason, it is clear that ray casting is a pure image-based volume rendering technique.

The basic idea that characterize ray casting is that starting from the view plane, one ray is cast forward and through the volume; as defined by Levoy, those rays should be parallel to each other.

Therefore, in order to be able to traverse the volumetric data, as the first phase of the proceed of the algorithm, it is necessary to compute the entry and the exit point, in the 3-Dimensional space, of each ray with respect to the position of the volume with respect to the world-space coordinates.%ref to word-space coordinates%
These exit and entry point, as will be described in the implementation chapter of this work, requires some collision detection calculation. 

The second step involves the sampling of the voxels inside the volume and this operation is performed at a constant step along the ray.

Color and opacity are sampled from the volume data in order to compute the final output view image and these values are sampled as emission values stored into the voxels, usually in the range of a 8 bit gray scale, from 0 to 255.

The next phase involves spending few more computational power in order to calculate a properly shadowed value of the color, from the previous step. This is a very fundamental part of the proceed in the medical imaging context as it allows a good, otherwise useless, visualization of the dataset.

In order to achieve the needed fine looking result, different approach can be taken involving some volumetric shading technique to approximate the light integral, such as Blinn-Phong global illumination model and approximation algorithm.

In the end, the final result is store into a 2-Dimensional image by means of compositing all the values sampled and transformed during the previous steps, into one single value per pixel. Usually such values are store as vectors that belongs to the RGB color space%ref to rgb chapter%
however as already seen in the color space chapter there exists different and more appropriate color space that doesn't encode the illumination inside the color itself.
In order to compose such values there exists different operators that can be applied. The most common one is the over operator proposed by Porter%cite porter
because it is also capable of dealing and preserving semi-transparency voxels values according to their opacity. More of these operators are describe in the Composition schemas section.

There are two ways of compositing, back-to-front and front-to-back. They both have their pros and cons: back-to-front is the most simple method, however it is also the most expensive one in terms of computational requirements,because it doesn't allows any kind of optimization as all the voxels, from the least visible to the closest and visible one, must be accounted; On the other hand, front-to-back method requires more storage memory as additional variable are needed to save characteristics such as transparency.

The volume ray casting technique proceed can be summarize as follows:
\begin{enumerate}
\item Define the number of ray according to the view screen size.
\item Before casting, compute the starting and exiting point as 3-Dimensional coordinates with respect to volume world-space position.
\item For each pixel of the view screen cast a ray through the volumetric data set.
\item At a constant step, sample voxels values of color and opacity and store them into the respective ray.
\item Transform each of these value by means of volumetric shading technique
\item Compose the final result back-to-front or front-to-back applying a chosen operator.
\end{enumerate}

Ray casting has different advantages, as it is simple and easy to implement in its very basic version, it allows different degrees of freedom in choosing which method or algorithm should be used during the different phases and its final result doesn't suffer of any artefacts that are present with other techniques instead.

However, there is a major cons while using this technique: its basic implementation is very slow and poor in terms of efficiency. This problem is the reason why it is not yet used in state-of-the-art medical imaging techniques even  it does produces very good results.

The aim of this work, as already stated, is exactly to build starting from scratch, a volume ray casting algorithm that can compete with nowadays state-of-the-art competitors exploiting modern graphics devices.

\subsubsection{Composition schemes}
%rifrasare
In this section the different number of composition schemes will be described and explained for the sake of completeness. that are used during the process of direct volume rendering and especially fundamental for the volume ray casting technique.

There exists a number of different composition schemes that can be used:
\begin{itemize}
\item Maximum intensity projection (MIP)
\item First hit
\item Accumulate
\item Average
\end{itemize}  
%inserire immagine
All of the above treat the sampled value of the voxel in different way. However they are used to extract the isosurfaces and to render them. There are not advantages or disadvantages on using one method or the other, because they are used to archive different results in terms of final image.
\paragraph{Maximum intensity projection} Maximum intensity projection is often used for magnetic resonance angiograms or to extract vessel structures from CT or MRT scans. For rendering, it uses the greatest value sampled from the ray and the final result can be seen in the Figure. 

%add image
\paragraph{First hit} First hit composite method works by means of using the sampled scalar value that is higher that a defined threshold limit. Then, the isosurface is extracted and rendered.

\paragraph{Accumulate} Accumulate compositing method is similar to the over operator. It works accumulating the intensity Color until enough opacity is collected. The final result will shown also transparent layer whom opacity is above 0 and 
therefore still visible.

\paragraph{Average} Average produces essentially the same picture of X-ray. Additionally, it does not take into account any opacity value.

\subsection{Volume rendering state-of-the-art} %TODO 26-28
%TODO scrivere una migliore introduzione pro 500 e pro 1000

In this section volume rendering techniques proceed and characteristics whom are used in state-of-the-art algorithms and systems deployed for rendering and creating interactive images of objects and phenomena represented as 3-Dimensional data,within the context of medical imaging and scientific visualization in general. Nowadays there exists different implementation of the previously described techniques that stands out in terms of output image quality in volume rendering. However, it is not only about high efficient software implementation, but also tangible hardware that is built to let the software part perfectly fits on and reach the best results.
The first system worth mentioning is VolumePro 500, built by Mitsubishi Electric company in 1999 based on the already existing architecture known as Cube-4. The system exploit the power capability of the shear-warping techniques with standard factorization to archive rendering of $256^3$ renders or smaller volumes at 30 frames per second.%TODO add ref 87,87
The system was subsequently acquired by TeraRecon Inc. which improved it and then release the VolumePro 1000 system. This enchanted version is a single chip for real-time volume rendering capable of processing volume data at a rate of $10^9$ samples per second, using a novel technique called Shear-Image order ray casting, that inherits the ray-per-pixel casting method of image-based ray casting technique, but introduces a new intermediate sample space in which the axes are sheared with respect to the image place(hence the name shear-image order). Both systems deploys a highly optimized memory address interface that enables efficiently read blocks and axis-aligned slices of voxels without wasting any more storage space.%TODO add ref 131
For the sake of clarity VolumePro 500 will not be discussed, instead a description of his successor VolumePro 1000 will be given along with its adopted approach, as they can be of much more interest for the readers and also because as the latter is the result of improvements of the first one, it is a better candidate to represent state-of-the-art implement in the direct volume rendering category. 
Another technology currently deployed as state-of-the-art volume rendering and real-time visualization is based on the well known Marching cubes technique and it is called VESTA. It belongs to 
%TODO search something about VESTA for surface exctracting algorithm
%TODO scrivere pro 500
\paragraph{VolumePro 1000} Volume Pro 1000 is developed and improved by TeraRecons Inc. which released the first version of the system in 2002.TeraRecons release is the successor of Volume Pro 500 which was built  the company Mitsubishi Electric and release it in 1999, improving an already existing architecture known as Cube-4, developed at SUNY Stony Brook.

The VolumePro 1000 system %TODO 131
uses a novel shear-image order ray-casting approach. %TODO ref visualization handbook 247
 Firstly, Volume Pro architecture deploy a special memory address arithmetic known as 3D skewing, as already said, let the system save storage memory. 
 The core idea is to casts rays directly through centers of pixels, as standard ray casting technique. However, it keeps the set of slices aligned to the base plane, very similar to 2D texture method. The 3D viewing transformation, or affine transformation, used as part of the base concept is decomposed in two different explicit matrices: one that transform from voxel coordinates space to an intermediate coordinate system called sample space; the second one that transform the depth values of the sampled points to reflect their distance from the image plane. %TODO pag 248 vis handbook
 These two transformation enables Volume Pro 1000 to produce final images that does not have to be warped because they are already aligned along viewing rays from image-plane pixels, eliminating the intermediate and the final shearing operations required by standard shear-warp technique. Therefore it produces images that are of higher quality than the ones produced by traditional shear-warping, like the one used in VolumePro 500 instead. %show figure 11.17

As well as it predecessor VolumePro 500, it performs a triliniear interpolation of the volume data and computes the gradient vector in hardware to prevent undersampling and artifacts popping out. It does so also reconnecting to the work of Rezk-Salama on 2D texture slicing, as it is possible to generate additional interpolated slices between original voxel an shift them with sub-pixel accuracy, artifacts keeping, in this way, ray and sample spacing constant. %TODO cire handbook

Another aspect that characterize VolumePro 1000 system is that Voxels can have up to four fields, each of them is associated with a lookup table. Those are organized in a set and used during the classification phase; they can be combined by means of a hierarchy of arithmetic-logic units as they are built in cascade.

The shear image proceed can be divided in two parts: The first part operates like the standard shear-warp technique by stepping through one slice of the volume at a time, that is stored in the main memory. Then, it interpolates adjacent voxel slices to obtain new slices called z-interpolated samples. These resulting slices will appear to be organized in a grid parallel to the x and y dimensions of the volume, as no other interpolation took place before.

The second part, traverse each of the new generate slice of z-interpolated sampled value. It enumerates the sample point on the rays that intersect with the slice and it computes the color and the opacity value of each one of the z-interpolated samples. It also associate a depth values to measure the distance from the sample to the eye and are used to embed polygons in the rendered images.
Finally those computed values are accumulated along the respective rays and a rendering of the volume is produced and stored directly on the image plane.
 

As shading technique, in VolumePro 1000  Phong shading illumination model is used, trying to ensure a correct gradient, and Compositing is made by means of alpha blending maximum intensity projection like  VolumePro 500, but its hardware also supports early ray termination that let it increase  its performance.

Part of this work is to be able to built a fully image-based algorithm based on the ray casting technique which reaches the degree of efficiency and quality in term of execution speed and output image quality similar to the ones reached when deploying VolumePro 1000. In the next section a fully detailed description of the architecture, technologies and concepts used in an attempt to reach such results.
\pagebreak
\section{Visualization algorithm implementation} 
\subsection{Technologies} %TODO 1
\subsection{Architecture} %TODO 2
\subsection{Basic volume ray casting} %TODO 2-4
\subsection{Parallelize with CUDA} %TODO 5-6
\section{Conclusions} 
\subsection{Rendering results} %TODO 7
\subsection{Results comparisons} %TODO 8
\subsection{Final considerations} %TODO 9
\subsection{Discussion} %TODO 9-10
\subsection{Outlook}
\section{Bibliography}
\end{document}
